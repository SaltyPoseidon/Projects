{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<small><font color=gray>Notebook author: <a href=\"https://www.linkedin.com/in/olegmelnikov/\" target=\"_blank\">Oleg Melnikov</a>\n",
        "\n",
        "**[<font size=6>üèÉHAR</font>](https://www.kaggle.com/competitions/29jan24hse-har/rules)**. [**Instructions**](https://colab.research.google.com/drive/1owkYjuRGkx050LQnM3b3yTzd0Dr2XbeV) for running Colabs."
      ],
      "metadata": {
        "id": "H211VO45y3re"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<small>**CONSENT.** <mark>[ X ]</mark> We consent to sharing our Colab (after the assignment ends) with other students/instructors for educational purposes."
      ],
      "metadata": {
        "id": "B9WbQS1vzgf5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CT3dcYSxxx_K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ba7a4b8-6df5-42e2-d169-ebffcb6d6408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive; drive.mount('/content/drive')   # OK to enable, if your kaggle.json is stored in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NZoMVT8krwWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade --force-reinstall --no-deps kaggle > log  # upgrade kaggle package (to avoid a warning)\n",
        "!mkdir -p ~/.kaggle                                           # .kaggle folder must contain kaggle.json for kaggle executable to properly authenticate you to Kaggle.com\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json >log  # First, download kaggle.json from kaggle.com (in Account page) and place it in the root of mounted Google Drive\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json > log                   # Alternative location of kaggle.json (without a connection to Google Drive)\n",
        "!chmod 600 ~/.kaggle/kaggle.json                              # give only the owner full read/write access to kaggle.json\n",
        "!kaggle config set -n competition -v 29jan24hse-har           # set the competition context for the next few kaggle API calls. !kaggle config view - shows current settings\n",
        "!kaggle competitions download >> log                          # download competition dataset as a zip file\n",
        "!unzip -o *.zip >> log                                        # Kaggle dataset is copied as a single file and needs to be unzipped.\n",
        "!kaggle competitions leaderboard --show                       # print public leaderboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuKXMRjsyxoK",
        "outputId": "ac39b6cc-d8eb-4c0c-d893-165d40784537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "- competition is now set to: 29jan24hse-har\n",
            "Using competition: 29jan24hse-har\n",
            "  teamId  teamName             submissionDate       score    \n",
            "--------  -------------------  -------------------  -------  \n",
            "11629292  Z                    2024-02-18 16:05:47  0.97284  \n",
            "11633526  Maxim Smeyanov       2024-02-18 01:36:03  0.96809  \n",
            "11578655  AI                   2024-02-18 09:45:17  0.96673  \n",
            "11651220  AD                   2024-02-18 11:40:18  0.96401  \n",
            "11610716  R                    2024-02-18 18:19:04  0.96334  \n",
            "11633731  S                    2024-02-18 15:52:03  0.96334  \n",
            "11588231  G                    2024-02-18 19:09:21  0.96130  \n",
            "11579168  A                    2024-02-18 18:26:44  0.95994  \n",
            "11633671  AA                   2024-02-18 17:23:40  0.95790  \n",
            "11591535  Artem Ilin           2024-02-03 14:46:22  0.95723  \n",
            "11591404  AF                   2024-02-18 18:39:01  0.95655  \n",
            "11619534  AJ                   2024-02-12 15:15:19  0.95519  \n",
            "11633717  AE                   2024-02-12 18:13:35  0.95315  \n",
            "11626825  X                    2024-02-18 10:38:33  0.95179  \n",
            "11596138  W                    2024-02-08 17:39:51  0.95112  \n",
            "11591981  AR                   2024-02-17 18:06:22  0.94976  \n",
            "11651999  Pavel Nedbay         2024-02-18 18:42:56  0.94908  \n",
            "11633549  Ilshat Dineev        2024-02-12 17:12:06  0.94840  \n",
            "11633581  Naumenko Daria       2024-02-18 17:27:47  0.94772  \n",
            "11633785  Zapriagaeva Vlada    2024-02-12 18:49:35  0.94704  \n",
            "11633038  AT                   2024-02-18 14:54:14  0.94704  \n",
            "11633425  O                    2024-02-12 16:51:20  0.94636  \n",
            "11634188  P                    2024-02-12 20:58:27  0.94636  \n",
            "11634201  Mikhail Gorodov      2024-02-12 21:01:39  0.94636  \n",
            "11634112  RomaKholinov         2024-02-12 20:23:37  0.94501  \n",
            "11637719  Ivan Sinitsin        2024-02-13 18:18:49  0.94501  \n",
            "11637738  Andre Lebedinskiy    2024-02-13 18:23:46  0.94501  \n",
            "11595367  BAKUROV Timofei      2024-02-03 13:38:09  0.94433  \n",
            "11637803  Y                    2024-02-18 10:31:47  0.94433  \n",
            "11634212  Aristarkh Borshchev  2024-02-12 21:06:25  0.94093  \n",
            "11633010  Alina Dekkusheva     2024-02-12 14:36:26  0.94025  \n",
            "11633569  IlyaKozhevnikov      2024-02-12 17:26:00  0.93686  \n",
            "11574845  Baseline             2024-01-29 19:58:15  0.92396  \n",
            "11631970  Denis Kulakov        2024-02-12 08:47:12  0.87508  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "%%capture\n",
        "%reset -f\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell as IS; IS.ast_node_interactivity = \"all\"\n",
        "import numpy as np, pandas as pd, time, os, random\n",
        "\n",
        "np.set_printoptions(linewidth=10000, precision=2, edgeitems=20, suppress=True)\n",
        "pd.set_option('display.max_colwidth', 1000, 'display.max_columns', 100, 'display.width', 1000, 'display.max_rows', 4)\n",
        "ToCSV = lambda df, fname: df.round(2).to_csv(f'{fname}.csv', index_label='id') # rounds values to 2 decimals\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from torchsummary import summary\n",
        "\n",
        "# Fallback options for TensorFlow + Keras and for SKLearn\n",
        "# import tensorflow as tf, tensorflow.keras as keras\n",
        "# from sklearn.neural_network import MLPClassifier   # SKLearn's MLP is optimised for CPU (and doesn't use GPU)\n",
        "# from keras.layers import Flatten, Dense\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "# os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "class Timer():\n",
        "  def __init__(self, lim:'RunTimeLimit'=60): self.t0, self.lim, _ = time.time(), lim, print(f'‚è≥ started. You have {lim} sec. Good luck!')\n",
        "  def ShowTime(self):\n",
        "    msg = f'Runtime is {time.time()-self.t0:.0f} sec'\n",
        "    print(f'\\033[91m\\033[1m' + msg + f' > {self.lim} sec limit!!!\\033[0m' if (time.time()-self.t0-1) > self.lim else msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOhT2o18yxld",
        "outputId": "63c2ec17-f980-4423-c257-b2ea6026e378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.9 s, sys: 365 ms, total: 2.27 s\n",
            "Wall time: 4.52 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Always seed your experiments\n",
        "def set_seed(seed: int = 42) -> None:\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # Set a fixed value for the hash seed\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    print(f\"Random seed set as {seed}\")\n",
        "\n",
        "set_seed(0)"
      ],
      "metadata": {
        "id": "nELogoFQ5sRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82fe7778-4f15-4323-9422-fa96c73f81bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed set as 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if cuda activated.\n",
        "# If not, go to Runtime -> Change runtime type. Select 'T4 GPU'\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print('cuda activated')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print('cpu activated')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ7jNmp-LYNB",
        "outputId": "37e08295-9bb0-4972-d340-d44f72353d5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda activated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%time vX  = pd.read_csv('testX.csv', index_col='id')  # load testing input features X (only)\n",
        "%time tYX = pd.read_csv('trainYX.csv')                # partially load training labels Y and input features X\n",
        "tYX  # 561 input features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "7VA1oBPyyxii",
        "outputId": "0ed43681-50a9-4021-9050-e3da9e87c2ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 244 ms, sys: 21.2 ms, total: 265 ms\n",
            "Wall time: 275 ms\n",
            "CPU times: user 53.4 s, sys: 5.86 s, total: 59.2 s\n",
            "Wall time: 1min 9s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        y       0       1       2       3       4       5       6       7       8       9      10      11      12      13      14      15      16      17      18      19      20      21      22      23      24      25      26      27      28      29      30      31      32      33      34      35      36      37      38      39      40      41      42      43     44      45      46      47      48  ...     511     512     513     514     515     516     517     518     519     520    521     522     523     524     525     526     527     528     529     530     531    532     533    534     535     536     537     538     539     540     541     542     543     544     545     546     547     548     549    550     551     552     553     554     555     556     557     558     559     560\n",
              "0       5  0.2778  0.0092 -0.0676 -0.9785 -0.9160 -0.9610 -0.9834 -0.9170 -0.9590 -0.9390 -0.4230 -0.7520  0.8496  0.6226  0.8400 -0.9434 -0.9614 -1.0370 -1.0150 -1.0070 -0.9640 -0.9550 -0.6772  0.0568  0.0192  0.5900 -0.3162  0.1833  0.4440 -0.2622  0.1092  0.4468 -0.4443 -0.1484  0.1718 -0.2727  0.0954 -0.4720 -0.5264  0.2332  0.9640 -0.1309  0.1071 -0.9814 -0.948 -0.9727 -0.9720 -0.9575 -0.9585  ... -0.9126 -0.2037 -0.5300 -0.8164 -0.9170 -0.8850 -0.9033 -0.9120 -0.9750 -0.9326 -1.014 -0.9560 -0.6780 -0.9966 -0.6180 -0.1021 -0.5977 -0.9546 -0.9110 -0.9260 -0.9297 -1.017 -0.9460 -1.022 -0.9570 -0.2930 -1.0100 -0.3455 -0.1411 -0.5215 -0.9585 -0.9160 -0.9434 -0.9414 -0.9750 -0.9414 -0.9890 -0.9610 -0.4453 -1.002 -0.5415 -0.0308 -0.5093  0.0380 -0.0912 -0.1415 -0.1316 -0.8200  0.1721 -0.0535\n",
              "1       1  0.2454  0.0073 -0.1046 -0.2010  0.1426 -0.2668 -0.2776  0.0648 -0.2605 -0.0572 -0.0364 -0.2830 -0.2830 -0.1448  0.4443 -0.0844 -0.6733 -0.7603 -0.7847 -0.4136 -0.3633 -0.1837  0.2830  0.5100  0.0582 -0.2502  0.3079 -0.1384  0.0822  0.0902 -0.0034  0.1969  0.0538  0.2996 -0.0258  0.0936 -0.3472 -0.1434 -0.4058  0.3690  0.9326 -0.2942 -0.0916 -0.9966 -0.964 -0.9663 -0.9746 -0.9736 -0.9634  ... -0.8115  0.4165 -0.4731 -0.8210  0.2542  0.2410  0.2688  0.0928 -0.7710  0.2430 -0.221 -0.1018  0.7134 -0.8994 -0.0642 -0.0842 -0.4750 -0.1345 -0.3853 -0.2573 -0.5430 -0.757 -0.1365 -0.677 -0.1826  0.6777 -0.7866  0.3240 -0.6206 -0.8530 -0.2500 -0.3025 -0.3176 -0.3198 -0.6426 -0.2488 -0.7236 -0.2512  0.6177 -0.910  0.1069 -0.0397 -0.4220  0.5480  0.6455  0.2296 -0.0335 -0.7000  0.2998  0.0880\n",
              "...    ..     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...     ...     ...     ...     ...  ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...     ...    ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...\n",
              "499998  4  0.2740 -0.0132 -0.1257 -0.9834 -1.0020 -0.9590 -0.9897 -0.9746 -0.9873 -0.9346 -0.5630 -0.8394  0.8306  0.6846  0.8350 -0.9840 -0.9824 -0.9960 -1.0200 -0.9950 -1.0160 -0.9590 -0.6500 -0.5225 -0.7974  0.5020 -0.2532  0.3723  0.1772  0.2920 -0.2756  0.3179 -0.1398  0.0948  0.0180 -0.1853  0.1871  0.0790 -0.0402 -0.0880  0.9785 -0.0442 -0.0532 -0.9950 -1.028 -0.9790 -1.0010 -0.9697 -1.0060  ... -0.2356  0.4312 -0.6030 -0.8706 -0.9700 -0.9863 -1.0010 -0.9990 -0.9927 -0.9910 -1.020 -1.0210 -0.9920 -0.9736  0.3857 -0.4620 -0.7485 -0.9985 -0.9575 -0.9897 -0.9814 -1.000 -0.9990 -1.008 -0.9660 -0.8574 -0.9210  0.1049 -0.6284 -0.8970 -1.0200 -1.0150 -0.9750 -1.0170 -0.9746 -0.9937 -0.9927 -0.9950 -1.0030 -0.844  0.2454 -0.3782 -0.7183 -0.0227  0.1957  0.1864  0.4556 -0.9326  0.1137  0.0595\n",
              "499999  5  0.2695 -0.0251 -0.1010 -1.0170 -0.9050 -0.9375 -0.9736 -0.8920 -0.9673 -0.9575 -0.5293 -0.8022  0.8530  0.6714  0.8480 -0.9624 -1.0205 -0.9900 -0.9600 -0.9960 -0.9480 -0.9720 -0.7320 -0.5117 -0.3535  0.3710 -0.2270  0.2700 -0.0636 -0.2438  0.0608  0.2050 -0.0218 -0.1199  0.0678  0.0154 -0.1132 -0.2886 -0.3882  0.6284  0.9966 -0.1277  0.0722 -1.0050 -0.925 -0.9440 -1.0050 -0.9824 -0.9233  ... -0.9500  0.0488 -0.3591 -0.7050 -1.0240 -0.9790 -0.9746 -0.9814 -0.9920 -0.9814 -1.013 -0.9860 -0.9650 -1.0150 -0.1430 -0.1555 -0.5180 -0.9320 -0.9200 -0.9424 -0.9326 -0.932 -0.9170 -0.985 -0.9463 -0.4020 -0.9640 -0.3160 -0.0948 -0.4695 -0.9590 -0.9500 -0.9976 -0.9680 -1.0340 -0.9727 -0.9900 -0.9790 -0.6980 -1.017 -0.4863  0.0084 -0.3293 -0.0127 -0.1399  0.4624 -0.7610 -0.8696  0.1720 -0.0272\n",
              "\n",
              "[500000 rows x 562 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ddd29f4d-9c37-42fd-8bda-90dedd1261fc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>...</th>\n",
              "      <th>511</th>\n",
              "      <th>512</th>\n",
              "      <th>513</th>\n",
              "      <th>514</th>\n",
              "      <th>515</th>\n",
              "      <th>516</th>\n",
              "      <th>517</th>\n",
              "      <th>518</th>\n",
              "      <th>519</th>\n",
              "      <th>520</th>\n",
              "      <th>521</th>\n",
              "      <th>522</th>\n",
              "      <th>523</th>\n",
              "      <th>524</th>\n",
              "      <th>525</th>\n",
              "      <th>526</th>\n",
              "      <th>527</th>\n",
              "      <th>528</th>\n",
              "      <th>529</th>\n",
              "      <th>530</th>\n",
              "      <th>531</th>\n",
              "      <th>532</th>\n",
              "      <th>533</th>\n",
              "      <th>534</th>\n",
              "      <th>535</th>\n",
              "      <th>536</th>\n",
              "      <th>537</th>\n",
              "      <th>538</th>\n",
              "      <th>539</th>\n",
              "      <th>540</th>\n",
              "      <th>541</th>\n",
              "      <th>542</th>\n",
              "      <th>543</th>\n",
              "      <th>544</th>\n",
              "      <th>545</th>\n",
              "      <th>546</th>\n",
              "      <th>547</th>\n",
              "      <th>548</th>\n",
              "      <th>549</th>\n",
              "      <th>550</th>\n",
              "      <th>551</th>\n",
              "      <th>552</th>\n",
              "      <th>553</th>\n",
              "      <th>554</th>\n",
              "      <th>555</th>\n",
              "      <th>556</th>\n",
              "      <th>557</th>\n",
              "      <th>558</th>\n",
              "      <th>559</th>\n",
              "      <th>560</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>0.2778</td>\n",
              "      <td>0.0092</td>\n",
              "      <td>-0.0676</td>\n",
              "      <td>-0.9785</td>\n",
              "      <td>-0.9160</td>\n",
              "      <td>-0.9610</td>\n",
              "      <td>-0.9834</td>\n",
              "      <td>-0.9170</td>\n",
              "      <td>-0.9590</td>\n",
              "      <td>-0.9390</td>\n",
              "      <td>-0.4230</td>\n",
              "      <td>-0.7520</td>\n",
              "      <td>0.8496</td>\n",
              "      <td>0.6226</td>\n",
              "      <td>0.8400</td>\n",
              "      <td>-0.9434</td>\n",
              "      <td>-0.9614</td>\n",
              "      <td>-1.0370</td>\n",
              "      <td>-1.0150</td>\n",
              "      <td>-1.0070</td>\n",
              "      <td>-0.9640</td>\n",
              "      <td>-0.9550</td>\n",
              "      <td>-0.6772</td>\n",
              "      <td>0.0568</td>\n",
              "      <td>0.0192</td>\n",
              "      <td>0.5900</td>\n",
              "      <td>-0.3162</td>\n",
              "      <td>0.1833</td>\n",
              "      <td>0.4440</td>\n",
              "      <td>-0.2622</td>\n",
              "      <td>0.1092</td>\n",
              "      <td>0.4468</td>\n",
              "      <td>-0.4443</td>\n",
              "      <td>-0.1484</td>\n",
              "      <td>0.1718</td>\n",
              "      <td>-0.2727</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>-0.4720</td>\n",
              "      <td>-0.5264</td>\n",
              "      <td>0.2332</td>\n",
              "      <td>0.9640</td>\n",
              "      <td>-0.1309</td>\n",
              "      <td>0.1071</td>\n",
              "      <td>-0.9814</td>\n",
              "      <td>-0.948</td>\n",
              "      <td>-0.9727</td>\n",
              "      <td>-0.9720</td>\n",
              "      <td>-0.9575</td>\n",
              "      <td>-0.9585</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.9126</td>\n",
              "      <td>-0.2037</td>\n",
              "      <td>-0.5300</td>\n",
              "      <td>-0.8164</td>\n",
              "      <td>-0.9170</td>\n",
              "      <td>-0.8850</td>\n",
              "      <td>-0.9033</td>\n",
              "      <td>-0.9120</td>\n",
              "      <td>-0.9750</td>\n",
              "      <td>-0.9326</td>\n",
              "      <td>-1.014</td>\n",
              "      <td>-0.9560</td>\n",
              "      <td>-0.6780</td>\n",
              "      <td>-0.9966</td>\n",
              "      <td>-0.6180</td>\n",
              "      <td>-0.1021</td>\n",
              "      <td>-0.5977</td>\n",
              "      <td>-0.9546</td>\n",
              "      <td>-0.9110</td>\n",
              "      <td>-0.9260</td>\n",
              "      <td>-0.9297</td>\n",
              "      <td>-1.017</td>\n",
              "      <td>-0.9460</td>\n",
              "      <td>-1.022</td>\n",
              "      <td>-0.9570</td>\n",
              "      <td>-0.2930</td>\n",
              "      <td>-1.0100</td>\n",
              "      <td>-0.3455</td>\n",
              "      <td>-0.1411</td>\n",
              "      <td>-0.5215</td>\n",
              "      <td>-0.9585</td>\n",
              "      <td>-0.9160</td>\n",
              "      <td>-0.9434</td>\n",
              "      <td>-0.9414</td>\n",
              "      <td>-0.9750</td>\n",
              "      <td>-0.9414</td>\n",
              "      <td>-0.9890</td>\n",
              "      <td>-0.9610</td>\n",
              "      <td>-0.4453</td>\n",
              "      <td>-1.002</td>\n",
              "      <td>-0.5415</td>\n",
              "      <td>-0.0308</td>\n",
              "      <td>-0.5093</td>\n",
              "      <td>0.0380</td>\n",
              "      <td>-0.0912</td>\n",
              "      <td>-0.1415</td>\n",
              "      <td>-0.1316</td>\n",
              "      <td>-0.8200</td>\n",
              "      <td>0.1721</td>\n",
              "      <td>-0.0535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2454</td>\n",
              "      <td>0.0073</td>\n",
              "      <td>-0.1046</td>\n",
              "      <td>-0.2010</td>\n",
              "      <td>0.1426</td>\n",
              "      <td>-0.2668</td>\n",
              "      <td>-0.2776</td>\n",
              "      <td>0.0648</td>\n",
              "      <td>-0.2605</td>\n",
              "      <td>-0.0572</td>\n",
              "      <td>-0.0364</td>\n",
              "      <td>-0.2830</td>\n",
              "      <td>-0.2830</td>\n",
              "      <td>-0.1448</td>\n",
              "      <td>0.4443</td>\n",
              "      <td>-0.0844</td>\n",
              "      <td>-0.6733</td>\n",
              "      <td>-0.7603</td>\n",
              "      <td>-0.7847</td>\n",
              "      <td>-0.4136</td>\n",
              "      <td>-0.3633</td>\n",
              "      <td>-0.1837</td>\n",
              "      <td>0.2830</td>\n",
              "      <td>0.5100</td>\n",
              "      <td>0.0582</td>\n",
              "      <td>-0.2502</td>\n",
              "      <td>0.3079</td>\n",
              "      <td>-0.1384</td>\n",
              "      <td>0.0822</td>\n",
              "      <td>0.0902</td>\n",
              "      <td>-0.0034</td>\n",
              "      <td>0.1969</td>\n",
              "      <td>0.0538</td>\n",
              "      <td>0.2996</td>\n",
              "      <td>-0.0258</td>\n",
              "      <td>0.0936</td>\n",
              "      <td>-0.3472</td>\n",
              "      <td>-0.1434</td>\n",
              "      <td>-0.4058</td>\n",
              "      <td>0.3690</td>\n",
              "      <td>0.9326</td>\n",
              "      <td>-0.2942</td>\n",
              "      <td>-0.0916</td>\n",
              "      <td>-0.9966</td>\n",
              "      <td>-0.964</td>\n",
              "      <td>-0.9663</td>\n",
              "      <td>-0.9746</td>\n",
              "      <td>-0.9736</td>\n",
              "      <td>-0.9634</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.8115</td>\n",
              "      <td>0.4165</td>\n",
              "      <td>-0.4731</td>\n",
              "      <td>-0.8210</td>\n",
              "      <td>0.2542</td>\n",
              "      <td>0.2410</td>\n",
              "      <td>0.2688</td>\n",
              "      <td>0.0928</td>\n",
              "      <td>-0.7710</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>-0.221</td>\n",
              "      <td>-0.1018</td>\n",
              "      <td>0.7134</td>\n",
              "      <td>-0.8994</td>\n",
              "      <td>-0.0642</td>\n",
              "      <td>-0.0842</td>\n",
              "      <td>-0.4750</td>\n",
              "      <td>-0.1345</td>\n",
              "      <td>-0.3853</td>\n",
              "      <td>-0.2573</td>\n",
              "      <td>-0.5430</td>\n",
              "      <td>-0.757</td>\n",
              "      <td>-0.1365</td>\n",
              "      <td>-0.677</td>\n",
              "      <td>-0.1826</td>\n",
              "      <td>0.6777</td>\n",
              "      <td>-0.7866</td>\n",
              "      <td>0.3240</td>\n",
              "      <td>-0.6206</td>\n",
              "      <td>-0.8530</td>\n",
              "      <td>-0.2500</td>\n",
              "      <td>-0.3025</td>\n",
              "      <td>-0.3176</td>\n",
              "      <td>-0.3198</td>\n",
              "      <td>-0.6426</td>\n",
              "      <td>-0.2488</td>\n",
              "      <td>-0.7236</td>\n",
              "      <td>-0.2512</td>\n",
              "      <td>0.6177</td>\n",
              "      <td>-0.910</td>\n",
              "      <td>0.1069</td>\n",
              "      <td>-0.0397</td>\n",
              "      <td>-0.4220</td>\n",
              "      <td>0.5480</td>\n",
              "      <td>0.6455</td>\n",
              "      <td>0.2296</td>\n",
              "      <td>-0.0335</td>\n",
              "      <td>-0.7000</td>\n",
              "      <td>0.2998</td>\n",
              "      <td>0.0880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499998</th>\n",
              "      <td>4</td>\n",
              "      <td>0.2740</td>\n",
              "      <td>-0.0132</td>\n",
              "      <td>-0.1257</td>\n",
              "      <td>-0.9834</td>\n",
              "      <td>-1.0020</td>\n",
              "      <td>-0.9590</td>\n",
              "      <td>-0.9897</td>\n",
              "      <td>-0.9746</td>\n",
              "      <td>-0.9873</td>\n",
              "      <td>-0.9346</td>\n",
              "      <td>-0.5630</td>\n",
              "      <td>-0.8394</td>\n",
              "      <td>0.8306</td>\n",
              "      <td>0.6846</td>\n",
              "      <td>0.8350</td>\n",
              "      <td>-0.9840</td>\n",
              "      <td>-0.9824</td>\n",
              "      <td>-0.9960</td>\n",
              "      <td>-1.0200</td>\n",
              "      <td>-0.9950</td>\n",
              "      <td>-1.0160</td>\n",
              "      <td>-0.9590</td>\n",
              "      <td>-0.6500</td>\n",
              "      <td>-0.5225</td>\n",
              "      <td>-0.7974</td>\n",
              "      <td>0.5020</td>\n",
              "      <td>-0.2532</td>\n",
              "      <td>0.3723</td>\n",
              "      <td>0.1772</td>\n",
              "      <td>0.2920</td>\n",
              "      <td>-0.2756</td>\n",
              "      <td>0.3179</td>\n",
              "      <td>-0.1398</td>\n",
              "      <td>0.0948</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>-0.1853</td>\n",
              "      <td>0.1871</td>\n",
              "      <td>0.0790</td>\n",
              "      <td>-0.0402</td>\n",
              "      <td>-0.0880</td>\n",
              "      <td>0.9785</td>\n",
              "      <td>-0.0442</td>\n",
              "      <td>-0.0532</td>\n",
              "      <td>-0.9950</td>\n",
              "      <td>-1.028</td>\n",
              "      <td>-0.9790</td>\n",
              "      <td>-1.0010</td>\n",
              "      <td>-0.9697</td>\n",
              "      <td>-1.0060</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.2356</td>\n",
              "      <td>0.4312</td>\n",
              "      <td>-0.6030</td>\n",
              "      <td>-0.8706</td>\n",
              "      <td>-0.9700</td>\n",
              "      <td>-0.9863</td>\n",
              "      <td>-1.0010</td>\n",
              "      <td>-0.9990</td>\n",
              "      <td>-0.9927</td>\n",
              "      <td>-0.9910</td>\n",
              "      <td>-1.020</td>\n",
              "      <td>-1.0210</td>\n",
              "      <td>-0.9920</td>\n",
              "      <td>-0.9736</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>-0.4620</td>\n",
              "      <td>-0.7485</td>\n",
              "      <td>-0.9985</td>\n",
              "      <td>-0.9575</td>\n",
              "      <td>-0.9897</td>\n",
              "      <td>-0.9814</td>\n",
              "      <td>-1.000</td>\n",
              "      <td>-0.9990</td>\n",
              "      <td>-1.008</td>\n",
              "      <td>-0.9660</td>\n",
              "      <td>-0.8574</td>\n",
              "      <td>-0.9210</td>\n",
              "      <td>0.1049</td>\n",
              "      <td>-0.6284</td>\n",
              "      <td>-0.8970</td>\n",
              "      <td>-1.0200</td>\n",
              "      <td>-1.0150</td>\n",
              "      <td>-0.9750</td>\n",
              "      <td>-1.0170</td>\n",
              "      <td>-0.9746</td>\n",
              "      <td>-0.9937</td>\n",
              "      <td>-0.9927</td>\n",
              "      <td>-0.9950</td>\n",
              "      <td>-1.0030</td>\n",
              "      <td>-0.844</td>\n",
              "      <td>0.2454</td>\n",
              "      <td>-0.3782</td>\n",
              "      <td>-0.7183</td>\n",
              "      <td>-0.0227</td>\n",
              "      <td>0.1957</td>\n",
              "      <td>0.1864</td>\n",
              "      <td>0.4556</td>\n",
              "      <td>-0.9326</td>\n",
              "      <td>0.1137</td>\n",
              "      <td>0.0595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499999</th>\n",
              "      <td>5</td>\n",
              "      <td>0.2695</td>\n",
              "      <td>-0.0251</td>\n",
              "      <td>-0.1010</td>\n",
              "      <td>-1.0170</td>\n",
              "      <td>-0.9050</td>\n",
              "      <td>-0.9375</td>\n",
              "      <td>-0.9736</td>\n",
              "      <td>-0.8920</td>\n",
              "      <td>-0.9673</td>\n",
              "      <td>-0.9575</td>\n",
              "      <td>-0.5293</td>\n",
              "      <td>-0.8022</td>\n",
              "      <td>0.8530</td>\n",
              "      <td>0.6714</td>\n",
              "      <td>0.8480</td>\n",
              "      <td>-0.9624</td>\n",
              "      <td>-1.0205</td>\n",
              "      <td>-0.9900</td>\n",
              "      <td>-0.9600</td>\n",
              "      <td>-0.9960</td>\n",
              "      <td>-0.9480</td>\n",
              "      <td>-0.9720</td>\n",
              "      <td>-0.7320</td>\n",
              "      <td>-0.5117</td>\n",
              "      <td>-0.3535</td>\n",
              "      <td>0.3710</td>\n",
              "      <td>-0.2270</td>\n",
              "      <td>0.2700</td>\n",
              "      <td>-0.0636</td>\n",
              "      <td>-0.2438</td>\n",
              "      <td>0.0608</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>-0.0218</td>\n",
              "      <td>-0.1199</td>\n",
              "      <td>0.0678</td>\n",
              "      <td>0.0154</td>\n",
              "      <td>-0.1132</td>\n",
              "      <td>-0.2886</td>\n",
              "      <td>-0.3882</td>\n",
              "      <td>0.6284</td>\n",
              "      <td>0.9966</td>\n",
              "      <td>-0.1277</td>\n",
              "      <td>0.0722</td>\n",
              "      <td>-1.0050</td>\n",
              "      <td>-0.925</td>\n",
              "      <td>-0.9440</td>\n",
              "      <td>-1.0050</td>\n",
              "      <td>-0.9824</td>\n",
              "      <td>-0.9233</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.9500</td>\n",
              "      <td>0.0488</td>\n",
              "      <td>-0.3591</td>\n",
              "      <td>-0.7050</td>\n",
              "      <td>-1.0240</td>\n",
              "      <td>-0.9790</td>\n",
              "      <td>-0.9746</td>\n",
              "      <td>-0.9814</td>\n",
              "      <td>-0.9920</td>\n",
              "      <td>-0.9814</td>\n",
              "      <td>-1.013</td>\n",
              "      <td>-0.9860</td>\n",
              "      <td>-0.9650</td>\n",
              "      <td>-1.0150</td>\n",
              "      <td>-0.1430</td>\n",
              "      <td>-0.1555</td>\n",
              "      <td>-0.5180</td>\n",
              "      <td>-0.9320</td>\n",
              "      <td>-0.9200</td>\n",
              "      <td>-0.9424</td>\n",
              "      <td>-0.9326</td>\n",
              "      <td>-0.932</td>\n",
              "      <td>-0.9170</td>\n",
              "      <td>-0.985</td>\n",
              "      <td>-0.9463</td>\n",
              "      <td>-0.4020</td>\n",
              "      <td>-0.9640</td>\n",
              "      <td>-0.3160</td>\n",
              "      <td>-0.0948</td>\n",
              "      <td>-0.4695</td>\n",
              "      <td>-0.9590</td>\n",
              "      <td>-0.9500</td>\n",
              "      <td>-0.9976</td>\n",
              "      <td>-0.9680</td>\n",
              "      <td>-1.0340</td>\n",
              "      <td>-0.9727</td>\n",
              "      <td>-0.9900</td>\n",
              "      <td>-0.9790</td>\n",
              "      <td>-0.6980</td>\n",
              "      <td>-1.017</td>\n",
              "      <td>-0.4863</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>-0.3293</td>\n",
              "      <td>-0.0127</td>\n",
              "      <td>-0.1399</td>\n",
              "      <td>0.4624</td>\n",
              "      <td>-0.7610</td>\n",
              "      <td>-0.8696</td>\n",
              "      <td>0.1720</td>\n",
              "      <td>-0.0272</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500000 rows √ó 562 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ddd29f4d-9c37-42fd-8bda-90dedd1261fc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ddd29f4d-9c37-42fd-8bda-90dedd1261fc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ddd29f4d-9c37-42fd-8bda-90dedd1261fc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8285b19f-4e59-45c8-85bc-2a522ab39b60\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8285b19f-4e59-45c8-85bc-2a522ab39b60')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8285b19f-4e59-45c8-85bc-2a522ab39b60 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "tYX"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tYX.y.value_counts(sort=False).to_frame().T  # counts of observations in each label category"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "5ip8I5XHzYfZ",
        "outputId": "797fa70e-5316-4bc5-e453-b0644cef7437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       5      1      3      4      2      6\n",
              "y  93667  83502  66901  87427  72554  95949"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-63121e80-e06e-4d0b-a6dc-abcd26385315\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>5</th>\n",
              "      <th>1</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>2</th>\n",
              "      <th>6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>y</th>\n",
              "      <td>93667</td>\n",
              "      <td>83502</td>\n",
              "      <td>66901</td>\n",
              "      <td>87427</td>\n",
              "      <td>72554</td>\n",
              "      <td>95949</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-63121e80-e06e-4d0b-a6dc-abcd26385315')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-63121e80-e06e-4d0b-a6dc-abcd26385315 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-63121e80-e06e-4d0b-a6dc-abcd26385315');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "cannot convert float NaN to integer"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tmr = Timer() # runtime limit (in seconds). Add all of your code after the timer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1l4r_NEyzZ_v",
        "outputId": "343065a3-b6c4-4559-a0be-cea840eebb6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ started. You have 60 sec. Good luck!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr color=red>\n",
        "\n",
        "<font size=5>‚è≥</font> <strong><font color=orange size=5>Your Code, Documentation, Ideas and Timer - All Start Here...</font></strong>\n",
        "\n",
        "Students: Keep all your definitions, code, documentation **between** ‚è≥ symbols."
      ],
      "metadata": {
        "id": "Vl15K7562P-n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpyLNt3god0c"
      },
      "source": [
        "## **Task 1. Preprocessing Pipeline**\n",
        "\n",
        "Explain elements of your preprocessing pipeline i.e. feature engineering, subsampling, clustering, dimensionality reduction, etc.\n",
        "1. Why did you choose these elements? (Something in EDA, prior experience,...? Btw, EDA is not required)\n",
        "2. How do you evaluate the effectiveness of these elements?\n",
        "3. What else have you tried that worked or didn't?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30xYIFXAnaPE"
      },
      "source": [
        "**Student's answer:**\n",
        "1. In preprocessing pipeline I used only sampling whole dataset, the optimal sampling was 50% of the original dataset\n",
        "2. Sampling increased speed of training of model and at the same time allowed to use more deep NN\n",
        "3. I tried to use PolynomialFeatures, but their computation and implementation in model increased timer too much. Also I tried PCA() but it didn't give huge advantage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGJRwzqHob4o"
      },
      "source": [
        "## **Task 2. Modeling Approach**\n",
        "Explain your modeling approach, i.e. ideas you tried and why you thought they would be helpful.\n",
        "\n",
        "1. How did these decisions guide you in modeling?\n",
        "1. How do you evaluate the effectiveness of these elements?\n",
        "1. What else have you tried that worked or didn't?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi6ZjgtWnb58"
      },
      "source": [
        "**Student's answer:**\n",
        "1. First of all I added more layers:\n",
        "- increased number of neurons\n",
        "- added additional ReLu\n",
        "- added additional dropout layers\n",
        "\n",
        "Also I made changing learning rate while training, it allowed to find good decision for first 3 epochs with 0.001 lr, then I started to decrease it and gave huge advantage in speed of training\n",
        "\n",
        "Also I changed optimizer to Adam\n",
        "\n",
        "2. Except for varying learning rate, I used acuracy and entropy for evaluating model perfomance\n",
        "I evaluated varying learning rate by difference of metrics at every epoch\n",
        "\n",
        "3. I tried to add less dropout layers, it increased score in the notebook, but in kaggle score was only 0.6, so I rejected this idea\n",
        "Also I made much deeper NN, but its training took to many time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tYX1 = tYX.sample(frac=0.5) # sampling as it recommended in instruction\n",
        "tX, tY = tYX1.drop('y', axis=1).head(50000), tYX1.head(50000).y-1   # shift labels by -1 to range {0,1,2,3,4,5}"
      ],
      "metadata": {
        "id": "cSKHErv_xQKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(ImprovedModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)  # –£–≤–µ–ª–∏—á–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤\n",
        "        self.dropout1 = nn.Dropout(0.5)  # –î–æ–±–∞–≤–ª–µ–Ω —Å–ª–æ–π Dropout\n",
        "        self.fc2 = nn.Linear(128, 64)  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Å–ª–æ–π\n",
        "        self.dropout2 = nn.Dropout(0.5)  # –î–æ–±–∞–≤–ª–µ–Ω —Å–ª–æ–π Dropout\n",
        "        self.fc3 = nn.Linear(64, 6)  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è 6 –∫–ª–∞—Å—Å–æ–≤\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Convert numpy arrays to torch tensors\n",
        "tX_tensor = torch.tensor(tX.values, dtype=torch.float32)\n",
        "tY_tensor = torch.tensor(tY.values, dtype=torch.long)\n",
        "\n",
        "# If using GPU\n",
        "tX_tensor = tX_tensor.to(device)\n",
        "tY_tensor = tY_tensor.to(device)\n",
        "\n",
        "# Create TensorDataset and split into train val sets\n",
        "dataset = TensorDataset(tX_tensor, tY_tensor)\n",
        "val_size = int(len(dataset) * 0.3)\n",
        "train_size = len(dataset) - val_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "# Instantiate the model\n",
        "model = ImprovedModel(input_size=tX.shape[1]).to(device)\n",
        "\n",
        "# Loss and optimizer with initial learning rate\n",
        "initial_lr = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "# Adjust learning rate at the 2nd\n",
        "    if epoch == 2:\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = 0.0001\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {np.round(running_loss/len(train_loader), 4)}\")\n",
        "\n",
        "    # Validation loop\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        print(f\"Validation Loss: {np.round(val_loss/len(val_loader), 4)}\")\n",
        "        print(f\"Accuracy: {np.round(100 * correct / total, 4)}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67E2C2GzaRFf",
        "outputId": "76d4e711-2d9d-49b3-d840-6b671f467e22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.4358\n",
            "Validation Loss: 0.2171\n",
            "Accuracy: 91.6067%\n",
            "Epoch 2/10, Loss: 0.1759\n",
            "Validation Loss: 0.1453\n",
            "Accuracy: 94.0933%\n",
            "Epoch 3/10, Loss: 0.1043\n",
            "Validation Loss: 0.0963\n",
            "Accuracy: 96.5733%\n",
            "Epoch 4/10, Loss: 0.0914\n",
            "Validation Loss: 0.09\n",
            "Accuracy: 96.76%\n",
            "Epoch 5/10, Loss: 0.0835\n",
            "Validation Loss: 0.0831\n",
            "Accuracy: 97.0867%\n",
            "Epoch 6/10, Loss: 0.0769\n",
            "Validation Loss: 0.0767\n",
            "Accuracy: 97.2533%\n",
            "Epoch 7/10, Loss: 0.0715\n",
            "Validation Loss: 0.0778\n",
            "Accuracy: 97.3533%\n",
            "Epoch 8/10, Loss: 0.0683\n",
            "Validation Loss: 0.0723\n",
            "Accuracy: 97.3733%\n",
            "Epoch 9/10, Loss: 0.0643\n",
            "Validation Loss: 0.0718\n",
            "Accuracy: 97.4867%\n",
            "Epoch 10/10, Loss: 0.0623\n",
            "Validation Loss: 0.0654\n",
            "Accuracy: 97.7067%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "shkBJlupkQGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming vX is a numpy array containing your validation data\n",
        "# You need to convert it into a torch tensor with the correct type\n",
        "vX_tensor = torch.tensor(vX.values, dtype=torch.float32)\n",
        "\n",
        "# If using GPU, send the model and the tensor to GPU\n",
        "model.to('cuda')\n",
        "vX_tensor = vX_tensor.to('cuda')\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# No need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    # Make predictions\n",
        "    predictions = model(vX_tensor)\n",
        "\n",
        "# If you need class probabilities, apply softmax\n",
        "probabilities = torch.softmax(predictions, dim=1)\n",
        "\n",
        "# To get the predicted class labels, get the index of the max log-probability\n",
        "predicted_labels = torch.max(probabilities, 1)[1]\n",
        "\n",
        "# Convert to numpy array if needed (for further processing in non-PyTorch code)\n",
        "probabilities_np = probabilities.cpu().numpy()\n",
        "predicted_labels_np = predicted_labels.cpu().numpy()\n",
        "\n",
        "# Now 'probabilities_np' holds class probabilities and 'predicted_labels_np' holds class predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8jgSQiPrBw_",
        "outputId": "df49ec9b-b4c3-4886-bf65-10977ebfbe2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ImprovedModel(\n",
              "  (fc1): Linear(in_features=561, out_features=128, bias=True)\n",
              "  (dropout1): Dropout(p=0.5, inplace=False)\n",
              "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (dropout2): Dropout(p=0.5, inplace=False)\n",
              "  (fc3): Linear(in_features=64, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ImprovedModel(\n",
              "  (fc1): Linear(in_features=561, out_features=128, bias=True)\n",
              "  (dropout1): Dropout(p=0.5, inplace=False)\n",
              "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (dropout2): Dropout(p=0.5, inplace=False)\n",
              "  (fc3): Linear(in_features=64, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "YLab = [f'{i}/{s}' for i, s in enumerate('walking walking_upstairs walking_downstairs sitting standing laying'.split())]  # column labels\n",
        "pd.DataFrame(probabilities_np[:3,:], columns=YLab).style.background_gradient(cmap='coolwarm', axis=1)  # display first few predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "_kDPxmFqaRNs",
        "outputId": "52d48d03-0379-42b1-ce2d-5a4e4ba046fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x79374df896f0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_1ae0d_row0_col0, #T_1ae0d_row1_col5, #T_1ae0d_row2_col5 {\n",
              "  background-color: #b40426;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_1ae0d_row0_col1, #T_1ae0d_row0_col2, #T_1ae0d_row0_col3, #T_1ae0d_row0_col4, #T_1ae0d_row0_col5, #T_1ae0d_row1_col0, #T_1ae0d_row1_col1, #T_1ae0d_row1_col2, #T_1ae0d_row1_col3, #T_1ae0d_row1_col4, #T_1ae0d_row2_col0, #T_1ae0d_row2_col1, #T_1ae0d_row2_col2, #T_1ae0d_row2_col3, #T_1ae0d_row2_col4 {\n",
              "  background-color: #3b4cc0;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_1ae0d\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_1ae0d_level0_col0\" class=\"col_heading level0 col0\" >0/walking</th>\n",
              "      <th id=\"T_1ae0d_level0_col1\" class=\"col_heading level0 col1\" >1/walking_upstairs</th>\n",
              "      <th id=\"T_1ae0d_level0_col2\" class=\"col_heading level0 col2\" >2/walking_downstairs</th>\n",
              "      <th id=\"T_1ae0d_level0_col3\" class=\"col_heading level0 col3\" >3/sitting</th>\n",
              "      <th id=\"T_1ae0d_level0_col4\" class=\"col_heading level0 col4\" >4/standing</th>\n",
              "      <th id=\"T_1ae0d_level0_col5\" class=\"col_heading level0 col5\" >5/laying</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_1ae0d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_1ae0d_row0_col0\" class=\"data row0 col0\" >0.999990</td>\n",
              "      <td id=\"T_1ae0d_row0_col1\" class=\"data row0 col1\" >0.000006</td>\n",
              "      <td id=\"T_1ae0d_row0_col2\" class=\"data row0 col2\" >0.000004</td>\n",
              "      <td id=\"T_1ae0d_row0_col3\" class=\"data row0 col3\" >0.000000</td>\n",
              "      <td id=\"T_1ae0d_row0_col4\" class=\"data row0 col4\" >0.000001</td>\n",
              "      <td id=\"T_1ae0d_row0_col5\" class=\"data row0 col5\" >0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_1ae0d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_1ae0d_row1_col0\" class=\"data row1 col0\" >0.000000</td>\n",
              "      <td id=\"T_1ae0d_row1_col1\" class=\"data row1 col1\" >0.000000</td>\n",
              "      <td id=\"T_1ae0d_row1_col2\" class=\"data row1 col2\" >0.000000</td>\n",
              "      <td id=\"T_1ae0d_row1_col3\" class=\"data row1 col3\" >0.000000</td>\n",
              "      <td id=\"T_1ae0d_row1_col4\" class=\"data row1 col4\" >0.000000</td>\n",
              "      <td id=\"T_1ae0d_row1_col5\" class=\"data row1 col5\" >1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_1ae0d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_1ae0d_row2_col0\" class=\"data row2 col0\" >0.000000</td>\n",
              "      <td id=\"T_1ae0d_row2_col1\" class=\"data row2 col1\" >0.000000</td>\n",
              "      <td id=\"T_1ae0d_row2_col2\" class=\"data row2 col2\" >0.000000</td>\n",
              "      <td id=\"T_1ae0d_row2_col3\" class=\"data row2 col3\" >0.000000</td>\n",
              "      <td id=\"T_1ae0d_row2_col4\" class=\"data row2 col4\" >0.000000</td>\n",
              "      <td id=\"T_1ae0d_row2_col5\" class=\"data row2 col5\" >1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = pd.DataFrame(predicted_labels_np + 1, columns=['y']) # labels are shifted to the initial state"
      ],
      "metadata": {
        "id": "JG6rFvOyir0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "hzGvrClxkB5y",
        "outputId": "444e16f1-79f2-4972-e54e-4d4d2a4e6dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   0     1     2     3     4     5     6     7     8     9     10    11    12    13    14    15    16    17    18    19    20    21    22    23    24    25    26    27    28    29    30    31    32    33    34    35    36    37    38    39    40    41    42    43    44    45    46    47    48    49    ...  2897  2898  2899  2900  2901  2902  2903  2904  2905  2906  2907  2908  2909  2910  2911  2912  2913  2914  2915  2916  2917  2918  2919  2920  2921  2922  2923  2924  2925  2926  2927  2928  2929  2930  2931  2932  2933  2934  2935  2936  2937  2938  2939  2940  2941  2942  2943  2944  2945  2946\n",
              "y     1     6     6     2     1     6     2     6     1     5     6     3     6     5     1     5     3     2     2     2     2     3     4     2     5     1     4     5     5     5     3     3     2     3     3     2     5     5     6     5     3     5     6     6     6     1     2     1     6     3  ...     2     5     4     5     2     3     2     6     1     5     1     5     2     3     4     4     2     2     1     3     1     5     5     2     6     3     6     6     6     3     6     5     3     5     1     4     1     3     6     6     2     1     3     2     4     5     6     3     4     2\n",
              "\n",
              "[1 rows x 2947 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-74cd3da1-6280-40a5-ab88-1602077103dd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>...</th>\n",
              "      <th>2897</th>\n",
              "      <th>2898</th>\n",
              "      <th>2899</th>\n",
              "      <th>2900</th>\n",
              "      <th>2901</th>\n",
              "      <th>2902</th>\n",
              "      <th>2903</th>\n",
              "      <th>2904</th>\n",
              "      <th>2905</th>\n",
              "      <th>2906</th>\n",
              "      <th>2907</th>\n",
              "      <th>2908</th>\n",
              "      <th>2909</th>\n",
              "      <th>2910</th>\n",
              "      <th>2911</th>\n",
              "      <th>2912</th>\n",
              "      <th>2913</th>\n",
              "      <th>2914</th>\n",
              "      <th>2915</th>\n",
              "      <th>2916</th>\n",
              "      <th>2917</th>\n",
              "      <th>2918</th>\n",
              "      <th>2919</th>\n",
              "      <th>2920</th>\n",
              "      <th>2921</th>\n",
              "      <th>2922</th>\n",
              "      <th>2923</th>\n",
              "      <th>2924</th>\n",
              "      <th>2925</th>\n",
              "      <th>2926</th>\n",
              "      <th>2927</th>\n",
              "      <th>2928</th>\n",
              "      <th>2929</th>\n",
              "      <th>2930</th>\n",
              "      <th>2931</th>\n",
              "      <th>2932</th>\n",
              "      <th>2933</th>\n",
              "      <th>2934</th>\n",
              "      <th>2935</th>\n",
              "      <th>2936</th>\n",
              "      <th>2937</th>\n",
              "      <th>2938</th>\n",
              "      <th>2939</th>\n",
              "      <th>2940</th>\n",
              "      <th>2941</th>\n",
              "      <th>2942</th>\n",
              "      <th>2943</th>\n",
              "      <th>2944</th>\n",
              "      <th>2945</th>\n",
              "      <th>2946</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>y</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows √ó 2947 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-74cd3da1-6280-40a5-ab88-1602077103dd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-74cd3da1-6280-40a5-ab88-1602077103dd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-74cd3da1-6280-40a5-ab88-1602077103dd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ToCSV(result, 'HAR201') # generate a CSV submission file for Kaggle"
      ],
      "metadata": {
        "id": "jXh8V5NbaRTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References:**"
      ],
      "metadata": {
        "id": "pzBsjCvS_kEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Remember to cite your sources here as well! At the least, your textbook should be cited. Google Scholar allows you to effortlessly copy/paste an APA citation format for books and publications. Also cite StackOverflow, package documentation, and other meaningful internet resources to help your peers learn from these (and to avoid plagiarism claims).\n",
        "1. ...\n",
        "1. ..."
      ],
      "metadata": {
        "id": "2kr8Q-9T_nAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=5>‚åõ</font> <strong><font color=orange size=5>Do not exceed competition's runtime limit!</font></strong>\n",
        "\n",
        "<hr color=red>\n"
      ],
      "metadata": {
        "id": "DoF2GoB_QGw9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nosV1OWFJPx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9039173-174b-4a21-c951-2629bc98227d"
      },
      "source": [
        "tmr.ShowTime()    # measure Colab's runtime. Do not remove. Keep as the last cell in your notebook."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runtime is 40 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udpl5HJ4JSLr"
      },
      "source": [
        "# üí°**Starter Ideas**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Try tuning DNN hyperparameters\n",
        "1. Training set has 500K observations (2GB), but you really don't need them all. They are all bootstrapped (with noise) from the original sample of 7352 observations. In order to stay within Colab runtime limit (CRTL), you can\n",
        "  1. use more observations for a shallow DNN, but risk underfitting due to lower model complexity\n",
        "  1. use fewer observations for a deeper DNN, but risk overfitting to higher model complexity\n",
        "1. Check out the original related papers about feature engineering for this dataset\n",
        "1. Try engineering features with [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) and discarding unimportant features via PCA or alternative technique.\n",
        "1. Consider KMeans/KMedoid or other clustering methods to identify observations, which represent the original 7352 observations. It might require finding 7352 cluster centroids/medoids.\n",
        "  1. Fast clustering methods: [FAISS](https://github.com/facebookresearch/faiss) (GPU-enabled)\n",
        "1. For deep NN, consider dropout, batch normalization\n",
        "1. Try PCA on transposed matrix to find/eliminate highly correlated observations\n",
        "1. Try [stratified sampling](https://en.wikipedia.org/wiki/Stratified_sampling) to ensure each label is proportionally represented in a subsample"
      ],
      "metadata": {
        "id": "_KOCCeay2r6G"
      }
    }
  ]
}